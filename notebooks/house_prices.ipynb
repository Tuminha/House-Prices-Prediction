{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfe0 House Prices - Advanced Regression Techniques\n\n**Learning Project**: Predicting house sale prices using Neural Networks  \n**Kaggle Competition**: [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n\n---\n\n## \ud83d\udcda What You'll Learn\n\nThis project builds on your Digit Recognizer experience but introduces **regression** instead of classification:\n\n| Concept | Classification (Digit Recognizer) | Regression (House Prices) |\n|---------|-----------------------------------|---------------------------|\n| **Output** | Class label (0-9) | Continuous value (price) |\n| **Output Layer** | 10 neurons (softmax) | 1 neuron (no activation) |\n| **Loss Function** | CrossEntropyLoss | MSELoss / L1Loss |\n| **Metrics** | Accuracy | RMSE, MAE, R\u00b2 |\n| **Prediction** | `torch.max(output, 1)` | `output.squeeze()` |\n\n---\n\n## \ud83d\udccb Project Structure\n\nThis notebook is divided into **7 phases**. Each phase contains:\n- \ud83d\udcd6 **Explanation** of concepts\n- \ud83c\udfaf **Learning objectives** for that phase\n- \u2705 **TODO blocks** where you'll write code\n- \ud83d\udca1 **Hints** to guide you (not complete solutions!)\n\nWork through each phase step by step. Ask for help if you get stuck!\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Environment Setup \u2705\n\n## \ud83c\udfaf Learning Objectives\n- Import necessary libraries for data science and deep learning\n- Check PyTorch installation and GPU availability\n- Load the dataset from CSV files\n- Understand the data structure\n\n## \ud83d\udcd6 Key Concepts\n\n**Libraries we'll use:**\n- `pandas` - Data manipulation and analysis\n- `numpy` - Numerical operations\n- `matplotlib` & `seaborn` - Data visualization\n- `torch` - Neural network framework\n- `sklearn` - Traditional ML algorithms and preprocessing tools\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1.1: Import Libraries\n# Import the following:\n# - pandas as pd\n# - numpy as np\n# - matplotlib.pyplot as plt\n# - seaborn as sns\n# - torch (PyTorch)\n# - torch.nn as nn\n# - torch.optim for optimizers\n\n# HINT: Use 'import X as Y' syntax for cleaner code\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1.2: Configure Display Settings\n# Set up nice display settings:\n# - Set seaborn style to 'darkgrid'\n# - Set matplotlib figure size to (12, 6) by default\n# - Set pandas display options to show all columns\n\n# HINT: Use sns.set_style(), plt.rcParams['figure.figsize'], pd.set_option()\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1.3: Check PyTorch Setup\n# Print the following:\n# - PyTorch version\n# - CUDA availability (GPU support)\n# - Device being used (cuda or cpu)\n\n# HINT: torch.__version__, torch.cuda.is_available(), torch.device()\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1.4: Load the Data\n# Load the training data from '../data/train.csv' into a DataFrame called 'train_df'\n# Load the test data from '../data/test.csv' into a DataFrame called 'test_df'\n# Display the first 5 rows of training data\n# Print the shape of both datasets\n\n# HINT: Use pd.read_csv(), .head(), .shape\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1.5: Basic Dataset Information\n# Display:\n# - Column names and data types using .info()\n# - Basic statistics using .describe()\n# - Number of numerical vs categorical columns\n\n# HINT: Use .info(), .describe(), .select_dtypes()\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Phase 1 Checklist\nBefore moving to Phase 2, make sure you've:\n- [ ] Imported all necessary libraries\n- [ ] Configured display settings\n- [ ] Checked PyTorch installation\n- [ ] Loaded both train and test datasets\n- [ ] Examined basic dataset information\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Exploratory Data Analysis (EDA) \ud83d\udd0d\n\n## \ud83c\udfaf Learning Objectives\n- Understand the distribution of the target variable (SalePrice)\n- Identify missing values in the dataset\n- Analyze correlations between features and target\n- Visualize key relationships\n- Identify important features for modeling\n\n## \ud83d\udcd6 Key Concepts\n\n**Why EDA matters:**\n- Understanding your data prevents modeling mistakes\n- Missing values need to be handled before training\n- Feature correlations help with feature selection\n- Outliers can hurt model performance\n\n**Important Note:** The competition metric is **RMSE of log(SalePrice)**, so we'll need to consider log transformation!\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2.1: Analyze the Target Variable\n# Create visualizations for SalePrice:\n# - Histogram with KDE\n# - Box plot to identify outliers\n# - Calculate and print basic statistics (mean, median, std, min, max)\n# - Check if the distribution is skewed (hint: .skew())\n\n# HINT: Use plt.subplot() to create multiple plots, sns.histplot(), sns.boxplot()\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2.2: Visualize Log-Transformed Target\n# Create a histogram of log(SalePrice) - this is what we'll actually predict!\n# Compare the skewness before and after log transformation\n# Use np.log1p() which is log(1+x) to handle any zeros safely\n\n# HINT: np.log1p(train_df['SalePrice']).hist()\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2.3: Missing Values Analysis\n# Calculate the percentage of missing values for each column\n# Display only columns with missing values, sorted by percentage (highest first)\n# Create a visualization showing missing value percentages\n\n# HINT: Use .isnull().sum(), calculate percentages, filter where > 0, sort_values()\n# For visualization: sns.barplot() works well\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2.4: Correlation Analysis\n# Calculate correlation of all NUMERICAL features with SalePrice\n# Display the top 10 most positively correlated features\n# Display the top 5 most negatively correlated features\n# Create a heatmap of correlations for the top 10 features\n\n# HINT: Select numerical columns using .select_dtypes(include=[np.number])\n# Use .corr() to get correlation matrix, then select 'SalePrice' column\n# For heatmap: sns.heatmap() with annot=True\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2.5: Visualize Key Relationships\n# Create scatter plots for the top 3 most correlated features vs SalePrice\n# Add trend lines to see the relationships clearly\n# Identify any outliers that might need handling\n\n# HINT: Use plt.subplot() to create 1x3 grid\n# sns.regplot() shows scatter + trend line\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2.6: Categorical Features Analysis\n# Identify all categorical features\n# For 2-3 interesting categorical features, create box plots showing SalePrice distribution by category\n# Examples: Neighborhood, OverallQual, HouseStyle\n\n# HINT: Use .select_dtypes(include=['object']) for categorical features\n# sns.boxplot() with x=categorical, y='SalePrice'\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Phase 2 Checklist\nBefore moving to Phase 3, make sure you've:\n- [ ] Analyzed SalePrice distribution and skewness\n- [ ] Identified all columns with missing values\n- [ ] Found the most correlated features with SalePrice\n- [ ] Created visualizations for key relationships\n- [ ] Identified potential outliers\n- [ ] Analyzed categorical features\n\n**Key Insights to Note:**\n- Which features have the highest correlation with SalePrice?\n- Which features have the most missing values?\n- Is the target variable skewed? (It should be - consider log transformation!)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Data Preprocessing \ud83d\udd04\n\n## \ud83c\udfaf Learning Objectives\n- Handle missing values with appropriate imputation strategies\n- Separate numerical and categorical features\n- Encode categorical variables for ML models\n- Handle outliers\n- Engineer new features from existing ones\n\n## \ud83d\udcd6 Key Concepts\n\n**Missing Value Strategies:**\n- Numerical: Mean, median, or specific value (e.g., 0 for missing garage size)\n- Categorical: Mode or 'None' category\n- Drop if >50% missing (be careful!)\n\n**Feature Engineering:**\nCreating new features can improve model performance significantly!\n- TotalSF = 1stFlrSF + 2ndFlrSF + TotalBsmtSF\n- TotalBath = FullBath + 0.5*HalfBath\n- HouseAge = YrSold - YearBuilt\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.1: Create a Copy for Processing\n# Create copies of train and test dataframes to preserve originals\n# We'll call them 'train' and 'test'\n\n# HINT: Use .copy() to avoid modifying original data\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.2: Save the Target Variable\n# Extract the target variable (SalePrice) from training data\n# Apply log transformation: y = np.log1p(SalePrice)\n# Drop SalePrice from the training dataframe\n# Store test IDs for later submission\n\n# HINT: y_train = np.log1p(train['SalePrice'])\n# test_ids = test['Id']\n# Use .drop() to remove columns\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.3: Handle Missing Values - Numerical Features\n# For numerical columns with missing values:\n# - LotFrontage: Fill with median\n# - GarageYrBlt: Fill with YearBuilt (makes sense - garage built with house)\n# - Garage features (GarageCars, GarageArea): Fill with 0 (no garage)\n# - Basement features (BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF): Fill with 0 (no basement)\n# - MasVnrArea: Fill with 0 (no masonry veneer)\n\n# HINT: Use .fillna() method\n# Apply same transformations to both train and test!\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.4: Handle Missing Values - Categorical Features\n# For categorical columns with missing values:\n# - For features where missing means 'None' (e.g., PoolQC, Fence, Alley), fill with 'None'\n# - For features where missing is random (e.g., Electrical), fill with mode (most common value)\n# - Check data description to understand what missing means for each feature!\n\n# HINT: Use .fillna('None') or .fillna(train[column].mode()[0])\n# Features that likely mean 'None': PoolQC, MiscFeature, Alley, Fence, FireplaceQu,\n#                                   GarageType, GarageFinish, GarageQual, GarageCond,\n#                                   BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2,\n#                                   MasVnrType\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.5: Feature Engineering\n# Create new features that might be useful:\n# - TotalSF: Total square footage (1stFlrSF + 2ndFlrSF + TotalBsmtSF)\n# - TotalBath: Total bathrooms (FullBath + 0.5*HalfBath + BsmtFullBath + 0.5*BsmtHalfBath)\n# - HouseAge: Age of house (YrSold - YearBuilt)\n# - RemodAge: Years since remodeling (YrSold - YearRemodAdd)\n# - TotalPorchSF: Total porch area (OpenPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch)\n\n# HINT: Simply add columns with arithmetic operations\n# train['TotalSF'] = train['1stFlrSF'] + train['2ndFlrSF'] + train['TotalBsmtSF']\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.6: Handle Outliers\n# Based on EDA, remove extreme outliers that don't make sense\n# Common outliers in this dataset:\n# - Houses with GrLivArea > 4000 but SalePrice < 300000 (huge house, low price - error?)\n# - You can visualize: plt.scatter(train['GrLivArea'], y_train)\n\n# HINT: Use boolean indexing to filter\n# Remember to also filter y_train to match!\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.7: Encode Categorical Variables\n# Use one-hot encoding (pd.get_dummies) to convert categorical variables to numbers\n# Apply to both train and test datasets\n# Use drop_first=True to avoid multicollinearity\n\n# HINT: train = pd.get_dummies(train, drop_first=True)\n# Make sure to apply to both train and test!\n# After encoding, train and test might have different columns - we'll handle this in next TODO\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.8: Align Train and Test Columns\n# After one-hot encoding, train and test might have different columns\n# Align them to have the same columns (use .align() method)\n# Fill any missing values with 0\n\n# HINT: train, test = train.align(test, join='left', axis=1, fill_value=0)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3.9: Final Verification\n# Print the shapes of train and test\n# Check for any remaining missing values\n# Print the number of features after preprocessing\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Phase 3 Checklist\nBefore moving to Phase 4, make sure you've:\n- [ ] Handled all missing values (both numerical and categorical)\n- [ ] Created new engineered features\n- [ ] Removed outliers\n- [ ] One-hot encoded all categorical variables\n- [ ] Aligned train and test columns\n- [ ] Verified no missing values remain\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 4: Feature Scaling & Selection \u2696\ufe0f\n\n## \ud83c\udfaf Learning Objectives\n- Understand why feature scaling is critical for neural networks\n- Apply StandardScaler to normalize features\n- Split data into training and validation sets\n- Convert data to PyTorch tensors\n\n## \ud83d\udcd6 Key Concepts\n\n**Why Scale Features?**\n- Neural networks train faster with scaled features\n- Features with large values can dominate the learning process\n- Standardization: (x - mean) / std \u2192 mean=0, std=1\n\n**Important:** \n- Fit scaler on training data only!\n- Transform both train and validation using the same scaler\n- Save the scaler for test predictions\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4.1: Import Scaling and Splitting Tools\n# Import:\n# - train_test_split from sklearn.model_selection\n# - StandardScaler from sklearn.preprocessing\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4.2: Train-Validation Split\n# Split your training data into train and validation sets\n# Use 80-20 split (test_size=0.2)\n# Set random_state=42 for reproducibility\n# Variables: X_train, X_val, y_train_split, y_val\n\n# HINT: from sklearn.model_selection import train_test_split\n# X_train, X_val, y_train_split, y_val = train_test_split(train, y_train, test_size=0.2, random_state=42)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4.3: Feature Scaling\n# Create a StandardScaler instance\n# Fit it on X_train only (never on validation or test!)\n# Transform X_train, X_val, and test using the fitted scaler\n# Store results in X_train_scaled, X_val_scaled, test_scaled\n\n# HINT: \n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_val_scaled = scaler.transform(X_val)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4.4: Convert to PyTorch Tensors\n# Convert all arrays to PyTorch tensors with dtype=torch.float32\n# Variables needed:\n# - X_train_tensor, X_val_tensor, test_tensor (features)\n# - y_train_tensor, y_val_tensor (targets)\n\n# HINT: torch.tensor(array, dtype=torch.float32)\n# For y, reshape to (n, 1) using .reshape(-1, 1)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4.5: Verify Tensor Shapes\n# Print the shapes of all tensors\n# Print the number of features (this is your input size for the network!)\n# Verify data types are float32\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Phase 4 Checklist\nBefore moving to Phase 5, make sure you've:\n- [ ] Split data into train and validation sets (80-20)\n- [ ] Scaled features using StandardScaler\n- [ ] Converted all data to PyTorch tensors (float32)\n- [ ] Verified tensor shapes are correct\n- [ ] Noted the number of input features for your network\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 5: Neural Network for Regression \ud83c\udfd7\ufe0f\n\n## \ud83c\udfaf Learning Objectives\n- Design a neural network architecture for regression\n- Understand the difference between classification and regression networks\n- Implement dropout for regularization\n- Choose appropriate loss function and optimizer\n\n## \ud83d\udcd6 Key Concepts\n\n**Regression vs Classification Network:**\n\n```python\n# Classification (10 classes):\nself.output = nn.Linear(64, 10)  # 10 neurons\n# No activation - CrossEntropyLoss includes softmax\n\n# Regression (continuous value):\nself.output = nn.Linear(64, 1)   # 1 neuron\n# No activation - we want raw continuous output\n```\n\n**Loss Functions for Regression:**\n- MSELoss (L2): Penalizes large errors heavily\n- L1Loss (MAE): More robust to outliers\n- HuberLoss: Combination of both\n\n**Suggested Architecture:**\n```\nInput (n features) \u2192 256 \u2192 ReLU \u2192 Dropout(0.2)\n                   \u2192 128 \u2192 ReLU \u2192 Dropout(0.2)\n                   \u2192 64  \u2192 ReLU\n                   \u2192 1   (output)\n```\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5.1: Define the Neural Network Class\n# Create a class called HousePricePredictor that inherits from nn.Module\n# Architecture:\n# - Input layer: takes n_features as input\n# - Hidden layer 1: 256 neurons + ReLU + Dropout(0.2)\n# - Hidden layer 2: 128 neurons + ReLU + Dropout(0.2)\n# - Hidden layer 3: 64 neurons + ReLU\n# - Output layer: 1 neuron (NO activation function!)\n\n# HINT: Similar to Digit Recognizer but:\n# - Output layer has 1 neuron instead of 10\n# - No activation on output layer\n# - Add dropout layers: nn.Dropout(0.2)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5.2: Initialize the Model\n# Create an instance of your HousePricePredictor\n# Pass the correct number of input features (from your tensors)\n# Move model to the appropriate device (GPU if available)\n# Print the model architecture\n\n# HINT: \n# n_features = X_train_tensor.shape[1]\n# model = HousePricePredictor(n_features).to(device)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5.3: Define Loss Function and Optimizer\n# Loss function: Use MSELoss (Mean Squared Error) for regression\n# Optimizer: Use Adam with learning_rate=0.001\n\n# HINT:\n# criterion = nn.MSELoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5.4: Count Parameters\n# Calculate and print the total number of trainable parameters in your model\n# This helps you understand model complexity\n\n# HINT: sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Phase 5 Checklist\nBefore moving to Phase 6, make sure you've:\n- [ ] Defined HousePricePredictor class with correct architecture\n- [ ] Output layer has 1 neuron (for regression)\n- [ ] Added dropout layers for regularization\n- [ ] Initialized model with correct input size\n- [ ] Defined MSELoss criterion\n- [ ] Initialized Adam optimizer\n- [ ] Counted model parameters\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 6: Training Pipeline \ud83d\ude82\n\n## \ud83c\udfaf Learning Objectives\n- Create DataLoaders for efficient batch training\n- Implement training loop with validation\n- Track regression metrics (MSE, RMSE, MAE, R\u00b2)\n- Visualize training progress\n- Save the best model\n\n## \ud83d\udcd6 Key Concepts\n\n**Regression Metrics:**\n- **MSE** (Mean Squared Error): Average of squared errors\n- **RMSE** (Root MSE): Square root of MSE - same units as target\n- **MAE** (Mean Absolute Error): Average of absolute errors\n- **R\u00b2** (R-squared): How much variance is explained (1.0 is perfect)\n\n**Training Process:**\n1. Forward pass: Get predictions\n2. Calculate loss\n3. Backward pass: Calculate gradients\n4. Update weights\n5. Validate and track metrics\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6.1: Import Additional Tools\n# Import:\n# - TensorDataset, DataLoader from torch.utils.data\n# - mean_squared_error, mean_absolute_error, r2_score from sklearn.metrics\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6.2: Create DataLoaders\n# Create TensorDatasets for train and validation\n# Create DataLoaders with batch_size=32\n# Shuffle training data, don't shuffle validation\n\n# HINT:\n# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6.3: Implement Training Loop\n# Create a training loop that:\n# - Trains for a specified number of epochs (start with 100)\n# - For each epoch:\n#   * Train on batches\n#   * Calculate training loss\n#   * Validate on validation set\n#   * Calculate validation metrics (MSE, RMSE, MAE, R\u00b2)\n#   * Track losses and metrics\n#   * Print progress every 10 epochs\n#   * Save best model based on validation RMSE\n\n# HINT: Similar to Digit Recognizer but:\n# - Use MSE loss instead of CrossEntropyLoss\n# - No torch.max() for predictions - just use output.squeeze()\n# - Calculate RMSE = sqrt(MSE)\n# - For R\u00b2: use sklearn.metrics.r2_score(y_true, y_pred)\n\n# Structure:\n# 1. Initialize lists to track metrics\n# 2. Set number of epochs\n# 3. For each epoch:\n#    a. Training phase (model.train())\n#    b. Validation phase (model.eval())\n#    c. Calculate and store metrics\n#    d. Save best model\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6.4: Visualize Training Progress\n# Create plots showing:\n# - Training and validation loss over epochs\n# - Validation RMSE over epochs\n# - Validation R\u00b2 over epochs\n\n# HINT: Use plt.subplot() to create multiple plots\n# Plot both train and val losses on same plot for comparison\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6.5: Evaluate Best Model\n# Load the best saved model\n# Evaluate on validation set\n# Print final metrics:\n# - Validation RMSE\n# - Validation MAE\n# - Validation R\u00b2\n\n# HINT: model.load_state_dict(torch.load('best_model.pth'))\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6.6: Prediction vs Actual Plot\n# Create a scatter plot of predicted vs actual values on validation set\n# Add a diagonal line showing perfect predictions\n# This visualizes how well the model performs\n\n# HINT:\n# plt.scatter(y_val_actual, y_val_pred, alpha=0.5)\n# plt.plot([min, max], [min, max], 'r--')  # diagonal line\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Phase 6 Checklist\nBefore moving to Phase 7, make sure you've:\n- [ ] Created DataLoaders for batching\n- [ ] Implemented complete training loop\n- [ ] Tracked training and validation losses\n- [ ] Calculated regression metrics (RMSE, MAE, R\u00b2)\n- [ ] Saved the best model\n- [ ] Visualized training progress\n- [ ] Created prediction vs actual plot\n\n**Target Performance:**\n- Validation RMSE < 0.15 (good)\n- Validation RMSE < 0.13 (great!)\n- R\u00b2 > 0.85 (good fit)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 7: Evaluation & Submission \ud83d\udcca\n\n## \ud83c\udfaf Learning Objectives\n- Generate predictions on test data\n- Create Kaggle submission file\n- Validate submission format\n- Document and save model\n- (Optional) Compare with traditional ML models\n\n## \ud83d\udcd6 Key Concepts\n\n**Important Steps:**\n1. Load best model\n2. Predict on test set (already scaled)\n3. **Reverse log transformation** (critical!)\n4. Create submission.csv\n5. Validate format\n\n**Remember:** We predicted log(SalePrice), so we need to reverse it:\n```python\npredictions = np.expm1(log_predictions)  # exp(x) - 1\n```\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 7.1: Load Best Model and Generate Test Predictions\n# Load your best saved model\n# Set model to eval mode\n# Generate predictions on test set\n# Remember to move test tensor to the same device as model\n\n# HINT:\n# model.load_state_dict(torch.load('best_model.pth'))\n# model.eval()\n# with torch.no_grad():\n#     predictions = model(test_tensor.to(device))\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 7.2: Reverse Log Transformation\n# Convert predictions from log scale back to actual prices\n# Use np.expm1() which is the inverse of np.log1p()\n# Convert to numpy array and flatten if needed\n\n# HINT:\n# predictions_log = predictions.cpu().numpy().flatten()\n# predictions_price = np.expm1(predictions_log)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 7.3: Create Submission File\n# Create a DataFrame with columns: ['Id', 'SalePrice']\n# Id should be from test_ids you saved earlier\n# SalePrice should be your predictions (in original scale!)\n# Save to '../submission.csv'\n\n# HINT:\n# submission = pd.DataFrame({\n#     'Id': test_ids,\n#     'SalePrice': predictions_price\n# })\n# submission.to_csv('../submission.csv', index=False)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 7.4: Validate Submission Format\n# Load the submission file and check:\n# - Columns are ['Id', 'SalePrice']\n# - Shape is (1459, 2)\n# - No missing values\n# - All prices are positive\n# - Display first few rows\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 7.5: Save Model and Metadata\n# Save:\n# - Model state dict to '../trained_models/house_price_model.pth'\n# - Model metadata (architecture, performance, date) to '../trained_models/model_metadata.json'\n# - Scaler object using joblib to '../trained_models/scaler.pkl'\n\n# HINT:\n# torch.save(model.state_dict(), '../trained_models/house_price_model.pth')\n# import json, joblib\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83c\udf89 Optional: Compare with Traditional ML Models\n\nWant to go further? Compare your neural network with traditional models!\n\n---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL TODO 7.6: Linear Regression Baseline\n# Train a simple Linear Regression model for comparison\n# Evaluate on validation set\n# Compare RMSE with your neural network\n\n# HINT:\n# from sklearn.linear_model import LinearRegression\n# lr = LinearRegression()\n# lr.fit(X_train_scaled, y_train_split)\n# predictions = lr.predict(X_val_scaled)\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL TODO 7.7: Random Forest Model\n# Train a Random Forest regressor\n# Evaluate and compare with neural network\n\n# HINT:\n# from sklearn.ensemble import RandomForestRegressor\n# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n# rf.fit(X_train_scaled, y_train_split.ravel())\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL TODO 7.8: Model Comparison Table\n# Create a comparison table showing:\n# Model | RMSE | MAE | R\u00b2 | Training Time\n# For: Neural Network, Linear Regression, Random Forest\n\n# Your code here:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Phase 7 Checklist\nBefore finalizing, make sure you've:\n- [ ] Generated predictions on test set\n- [ ] Reversed log transformation\n- [ ] Created submission.csv\n- [ ] Validated submission format\n- [ ] Saved model and metadata\n- [ ] (Optional) Compared with traditional ML models\n\n---\n\n## \ud83d\ude80 Next Steps\n\n1. **Submit to Kaggle:**\n   - Go to [competition page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n   - Click \"Submit Predictions\"\n   - Upload your `submission.csv`\n\n2. **Update README.md:**\n   - Add your final results\n   - Include visualizations\n   - Document your learning journey\n\n3. **Create MODEL_CARD.md:**\n   - Document architecture\n   - Performance metrics\n   - Training details\n\n4. **Git Commit:**\n   - Commit your completed notebook\n   - Push to GitHub\n\n---\n\n## \ud83c\udf93 What You've Learned\n\nCongratulations! Through this project, you've learned:\n\n\u2705 **Regression with Neural Networks**\n- Output layer design for continuous predictions\n- Appropriate loss functions (MSELoss)\n- Regression metrics (RMSE, MAE, R\u00b2)\n\n\u2705 **Feature Engineering**\n- Creating new features from existing ones\n- Feature scaling and normalization\n- Handling mixed data types\n\n\u2705 **Data Preprocessing**\n- Missing value imputation strategies\n- One-hot encoding categorical variables\n- Outlier detection and handling\n\n\u2705 **Model Evaluation**\n- Proper train/validation split\n- Tracking multiple metrics\n- Comparing different model types\n\n\u2705 **Production Skills**\n- Saving models and scalers for deployment\n- Creating submission files for competitions\n- Documenting models with metadata\n\n---\n\n## \ud83d\udcaa Keep Learning!\n\nReady for more challenges?\n- Try ensemble methods (combining multiple models)\n- Experiment with feature selection techniques\n- Learn about XGBoost and LightGBM\n- Explore AutoML tools\n\nGreat job! \ud83c\udf89"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}